{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":239082875,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch transformers langchain gradio sacremoses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:56:47.304456Z","iopub.execute_input":"2025-05-12T19:56:47.304776Z","iopub.status.idle":"2025-05-12T19:56:51.422041Z","shell.execute_reply.started":"2025-05-12T19:56:47.304750Z","shell.execute_reply":"2025-05-12T19:56:51.420882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:56:51.423793Z","iopub.execute_input":"2025-05-12T19:56:51.424114Z","iopub.status.idle":"2025-05-12T19:56:51.906796Z","shell.execute_reply.started":"2025-05-12T19:56:51.424085Z","shell.execute_reply":"2025-05-12T19:56:51.906072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\nimport gradio as gr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:56:51.908137Z","iopub.execute_input":"2025-05-12T19:56:51.908724Z","iopub.status.idle":"2025-05-12T19:56:55.387003Z","shell.execute_reply.started":"2025-05-12T19:56:51.908690Z","shell.execute_reply":"2025-05-12T19:56:55.385852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BioGptForCausalLM, BioGptTokenizer,pipeline\nimport torch\nfrom peft import PeftModel\n\nbase_model = BioGptForCausalLM.from_pretrained(\n    \"microsoft/biogpt\",  # Original base model\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\nmodel = PeftModel.from_pretrained(base_model, \"/kaggle/input/notebook36cd12f0a1/biogpt-qlora-medical\")\nmodel = model.merge_and_unload() \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:56:55.388879Z","iopub.execute_input":"2025-05-12T19:56:55.389428Z","iopub.status.idle":"2025-05-12T19:57:06.878069Z","shell.execute_reply.started":"2025-05-12T19:56:55.389400Z","shell.execute_reply":"2025-05-12T19:57:06.877147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_from_disk\n\nval_dataset = load_from_disk(\"/kaggle/input/notebook36cd12f0a1/val_dataset\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:57:06.878985Z","iopub.execute_input":"2025-05-12T19:57:06.879774Z","iopub.status.idle":"2025-05-12T19:57:07.089100Z","shell.execute_reply.started":"2025-05-12T19:57:06.879745Z","shell.execute_reply":"2025-05-12T19:57:07.088142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U langchain langchain-core langchain-community langchain-huggingface\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:57:07.090180Z","iopub.execute_input":"2025-05-12T19:57:07.091530Z","iopub.status.idle":"2025-05-12T19:57:11.822495Z","shell.execute_reply.started":"2025-05-12T19:57:07.091498Z","shell.execute_reply":"2025-05-12T19:57:11.821292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_huggingface import HuggingFacePipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:57:11.823756Z","iopub.execute_input":"2025-05-12T19:57:11.824091Z","iopub.status.idle":"2025-05-12T19:57:11.934794Z","shell.execute_reply.started":"2025-05-12T19:57:11.824048Z","shell.execute_reply":"2025-05-12T19:57:11.933794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\") \npipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=150,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.6,\n    repetition_penalty=1.1,\n    pad_token_id=tokenizer.eos_token_id,\n    prefix=\"\",\n)\n\nfrom langchain_core.prompts.prompt import PromptTemplate\n\ntemplate = \"\"\"\nHistory:{history}\n###Patient: {input}\n###Doctor:\"\"\"\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:57:11.935870Z","iopub.execute_input":"2025-05-12T19:57:11.936302Z","iopub.status.idle":"2025-05-12T19:57:14.448731Z","shell.execute_reply.started":"2025-05-12T19:57:11.936263Z","shell.execute_reply":"2025-05-12T19:57:14.447543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Langchain memory and conversation chain\nPROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\nmemory = ConversationBufferMemory(k=2, return_messages=True,ai_prefix=\"Doctor\",human_prefix=\"Patient\")\nllm = HuggingFacePipeline(pipeline=pipeline)\n\nconversation = ConversationChain(llm=llm, memory=memory,prompt=PROMPT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:57:14.450045Z","iopub.execute_input":"2025-05-12T19:57:14.450390Z","iopub.status.idle":"2025-05-12T19:57:14.461026Z","shell.execute_reply.started":"2025-05-12T19:57:14.450363Z","shell.execute_reply":"2025-05-12T19:57:14.460037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.schema import HumanMessage, AIMessage\n\ndef extract_latest_doctor_response(text):\n    last_index = text.rfind(\"Doctor:\")\n    if last_index != -1:\n        return text[last_index + len(\"Doctor:\"):].strip()\n    else:\n        return text.strip()\n        \ndef format_history(messages):\n    last_two_messages = messages[-2:]\n\n    history_lines = []\n    for msg in last_two_messages:\n        if isinstance(msg, HumanMessage):\n            history_lines.append(f\"Patient: {msg.content.strip()}\")\n        elif isinstance(msg, AIMessage):\n            history_lines.append(f\"Doctor: {msg.content.strip()}\")\n    \n    return \"\\n\".join(history_lines)\n\n\ndef predict(input, history=None):\n    formatted_history = format_history(memory.chat_memory.messages)\n\n    prompt = PROMPT.format(history=formatted_history, input=input)\n\n    raw_response = llm(prompt)\n\n    cleaned_response = extract_latest_doctor_response(raw_response)\n\n    memory.chat_memory.add_user_message(input)\n    memory.chat_memory.add_ai_message(cleaned_response)\n\n    return raw_response\n\n\n# Create the Gradio UI\niface = gr.ChatInterface(\n    fn=predict,\n    title=\"BioGPT Doctor Chatbot\",\n    description=\"Ask BioGPT medical-related questions.\",\n    examples=[\n        [\"My knees are experiencing alot of pain.\"],\n        [\"I have a persistent cough and fever.\"],\n        [\"What are the common symptoms of diabetes?\"],\n    ],\n    theme=\"soft\",\n)\n\niface.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:57:14.463957Z","iopub.execute_input":"2025-05-12T19:57:14.464364Z","iopub.status.idle":"2025-05-12T19:57:18.361339Z","shell.execute_reply.started":"2025-05-12T19:57:14.464340Z","shell.execute_reply":"2025-05-12T19:57:18.360348Z"}},"outputs":[],"execution_count":null}]}